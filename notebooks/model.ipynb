{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21868cd",
   "metadata": {},
   "source": [
    "# 모델 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "be3873f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6caef9",
   "metadata": {},
   "source": [
    "## Global State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "79c8e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "import pandas as pd\n",
    "from dataclasses import field\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    \n",
    "    # 입력된 데이터\n",
    "    input_id: str = ''  \n",
    "    input_category: str = ''\n",
    "    \n",
    "    # 메타데이터 검색을 통해 확인한 제목과 설명\n",
    "    title: str = ''   \n",
    "    description: str = ''\n",
    "    keyword: str = ''\n",
    "    \n",
    "    # LLM이 생성한 검색어 리스트\n",
    "    query : list[str] = []\n",
    "    \n",
    "    # 검색어를 통해 검색한 전체 데이터\n",
    "    search_df: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "\n",
    "    # 연관성 검색을 통해 확인한 topK 데이터\n",
    "    relevance_df: pd.DataFrame = field(default_factory=pd.DataFrame)\n",
    "    \n",
    "    # 결과\n",
    "    result_df: pd.DataFrame = field(default_factory=pd.DataFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363ef43",
   "metadata": {},
   "source": [
    "## 논문 - API 호출용 토큰 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "bf99e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, base64, requests\n",
    "from Crypto.Cipher import AES\n",
    "from datetime import datetime\n",
    "\n",
    "class AESTestClass:\n",
    "    def __init__(self, plain_txt, key):\n",
    "        # iv, block_size 값은 고정\n",
    "        self.iv = 'jvHJ1EFA0IXBrxxz'\n",
    "        self.block_size = 16\n",
    "        self.plain_txt = plain_txt\n",
    "        self.key = key\n",
    "\n",
    "    def pad(self):\n",
    "        # PKCS#7 패딩\n",
    "        number_of_bytes_to_pad = self.block_size - len(self.plain_txt) % self.block_size\n",
    "        ascii_str = chr(number_of_bytes_to_pad)\n",
    "        padding_str = number_of_bytes_to_pad * ascii_str\n",
    "        return self.plain_txt + padding_str\n",
    "\n",
    "    def encrypt(self):\n",
    "        cipher = AES.new(self.key.encode('utf-8'), AES.MODE_CBC, self.iv.encode('utf-8'))\n",
    "        padded_txt = self.pad()\n",
    "        encrypted_bytes = cipher.encrypt(padded_txt.encode('utf-8'))\n",
    "        # URL-safe Base64\n",
    "        encrypted_str = base64.urlsafe_b64encode(encrypted_bytes).decode('utf-8')\n",
    "        return encrypted_str\n",
    "    \n",
    "def call_access_token(MAC_ADDRESS, API_KEY, CLIENT_ID):\n",
    "    # 맥주소\n",
    "    mac = (MAC_ADDRESS or \"\").strip().strip('\"').strip(\"'\").upper().replace(\":\", \"-\")\n",
    "    if not mac:\n",
    "        raise SystemExit(\"MAC_ADDRESS가 비어있음\")\n",
    "\n",
    "    # datetime 생성\n",
    "    dt = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "    # JSON 페이로드 생성\n",
    "    payload = {\n",
    "        \"mac_address\": mac,\n",
    "        \"datetime\": dt\n",
    "    }\n",
    "    plain_json = json.dumps(payload, separators=(',', ':'))\n",
    "\n",
    "    # AES 암호화 → Base64\n",
    "    aes = AESTestClass(plain_txt=plain_json, key=API_KEY)\n",
    "    b64_cipher = aes.encrypt()\n",
    "\n",
    "    # 인코딩 + 토큰 요청\n",
    "    endpoint = \"https://apigateway.kisti.re.kr/tokenrequest.do\"\n",
    "    params = {\n",
    "        \"accounts\": b64_cipher,\n",
    "        \"client_id\": CLIENT_ID\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint, params=params, timeout=10)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "    token = data['access_token'] \n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c640022",
   "metadata": {},
   "source": [
    "## 논문 - 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "bd644979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def xml_to_df(xml):\n",
    "    # XML 파싱\n",
    "    root = ET.fromstring(xml)\n",
    "\n",
    "    # recordList 찾기\n",
    "    record_list_element = root.find('recordList')\n",
    "\n",
    "    # 데이터를 담을 리스트\n",
    "    records = []\n",
    "\n",
    "    if record_list_element is not None:\n",
    "        # 각 record에 대해 반복\n",
    "        for record_element in record_list_element.findall('record'):\n",
    "            record_data = {}\n",
    "            # 각 item에 대해 반복\n",
    "            for item_element in record_element.findall('item'):\n",
    "                meta_code = item_element.get('metaCode')\n",
    "                # CDATA 섹션의 텍스트 추출\n",
    "                value = item_element.text.strip() if item_element.text else ''\n",
    "                record_data[meta_code] = value\n",
    "            records.append(record_data)\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "def transform_query(input_query):\n",
    "\n",
    "    query = {\n",
    "        \"BI\": input_query,  # 전체\n",
    "        # \"TI\": None,  # 논문명\n",
    "        # \"AU\": None,  # 저자\n",
    "        # \"AB\": None,  # 초록\n",
    "        # \"KW\": None,  # 키워드\n",
    "        # \"PB\": None,  # 출판사(발행기관)\n",
    "        # \"SN\": None,  # ISSN\n",
    "        # \"BN\": None,  # ISBN\n",
    "        # \"PY\": None,  # 발행년도\n",
    "        # \"CN\": None,  # 문헌번호\n",
    "        # \"DI\": None   # DOI\n",
    "    }\n",
    "\n",
    "    json_query = json.dumps(query, separators=(',', ':')) \n",
    "\n",
    "    return json_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbc843",
   "metadata": {},
   "source": [
    "## 논문 - 제목, 초록 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d6e7dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, xmltodict\n",
    "\n",
    "def ARTI_browse(state: State):\n",
    "    \n",
    "    CLIENT_ID = os.getenv(\"SCIENCEON_CLIENT_ID\")\n",
    "    ARTI_KEY = os.getenv(\"SCIENCEON_API_KEY\")\n",
    "    MAC_ADDRESS = os.getenv(\"MAC_ADDRESS\")\n",
    "\n",
    "    access_token = call_access_token(MAC_ADDRESS, ARTI_KEY, CLIENT_ID)\n",
    "\n",
    "    url = \"https://apigateway.kisti.re.kr/openapicall.do\"\n",
    "    params = {\n",
    "        \"client_id\": CLIENT_ID,\n",
    "        \"token\": access_token,\n",
    "        \"version\": 1.0,\n",
    "        \"action\": \"browse\",\n",
    "        \"target\": \"ARTI\",\n",
    "        \"cn\": state['input_id'],\n",
    "        \"include\": \"\",\n",
    "        \"exclude\": None,\n",
    "    }\n",
    "    \n",
    "    res = requests.get(url, params=params, timeout=20)\n",
    "    xml = res.text\n",
    "    dict_data = xmltodict.parse(xml)\n",
    "    with open(\"../data/input_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dict_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    df = xml_to_df(xml)\n",
    "    \n",
    "    title, description, keyword = df['Title'].iloc[0], df['Abstract'].iloc[0], df['Keyword'].iloc[0]\n",
    "    \n",
    "    print('\\n[title]\\n', title)\n",
    "    print('\\n[description]\\n', description)\n",
    "    print('\\n[keyword]\\n', keyword)    \n",
    "\n",
    "    return {'title': title, 'description': description, 'keyword': keyword}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62813e19",
   "metadata": {},
   "source": [
    "## 데이터셋 - 제목, 설명 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "f909f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, json\n",
    "\n",
    "def DATA_browse(state: State):\n",
    "\n",
    "    API_KEY = os.getenv(\"DATAON_META_API_KEY\")\n",
    "    assert API_KEY and API_KEY.strip(), \"환경변수(DATAON_META_API_KEY)가 비어있어요!\"\n",
    "\n",
    "    url = \"https://dataon.kisti.re.kr/rest/api/search/dataset/\" + state[\"input_id\"]\n",
    "    params = {\"key\": API_KEY}\n",
    "\n",
    "    res = requests.get(url, params=params, timeout=20)\n",
    "    data = res.json()\n",
    "\n",
    "    # json 저장\n",
    "    with open(\"../data/input_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data['records'], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    title, description, keyword = data['records']['dataset_title_etc_main'], data['records']['dataset_expl_etc_main'], data['records']['dataset_kywd_etc_main']\n",
    "    print('\\n[title]\\n', title)\n",
    "    print('\\n[description]\\n', description)\n",
    "    print('\\n[keyword]\\n', keyword)\n",
    "\n",
    "    return {'title':title, 'description' : description, 'keyword': keyword} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb6461",
   "metadata": {},
   "source": [
    "## 검색어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf05987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing_extensions import Annotated\n",
    "from pydantic import Field, BaseModel\n",
    "\n",
    "# Schema\n",
    "class QueryResult(BaseModel):\n",
    "    query: Annotated[\n",
    "        list[str],\n",
    "        Field(\n",
    "            ..., \n",
    "            max_length=5, \n",
    "            min_length=3,\n",
    "            description=\"가장 적절한 검색어들의 리스트, 길이 최소 3개/최대 5개\", \n",
    "        )\n",
    "    ]\n",
    "    \n",
    "# Prompt\n",
    "query_template = '''\n",
    "주어진 제목과 설명을 바탕으로, 의미적으로 가장 관련성이 높은 논문과 데이터셋을 찾기 위한 검색어를 생성하세요.\n",
    "\n",
    "[검색 엔진 제약]\n",
    "검색은 정확 일치 기반입니다. 쿼리는 짧고 응집력 있게 만드세요.\n",
    "각 쿼리는 2~3단어, 고유명사나 기술 토픽의 조합을 선호합니다.\n",
    "\n",
    "[생성 절차]\n",
    "1. 주제 핵심어 파악: 연구 주제의 주요 객체, 방법론, 도메인, 응용 맥락을 한 문장으로 요약\n",
    "2. 후보 생성: 2~3단어 쿼리 후보를 8~10개 잠정 생성\n",
    "3. 필터링 규칙 적용:\n",
    "  - 일반어, 지나치게 포괄적이거나 모호한 표현 제거(ex: “AI model”, “data analysis”)\n",
    "  - 12개는 방법론 중심, 12개는 도메인 중심, 1~2개는 응용 시나리오 중심으로 균형 있게 남김\n",
    "  - 약어만 있는 경우는 배제하되, 널리 쓰이는 고유 약어는 유지(ex: “BERT” 가능, “ML” 단독 불가)\n",
    "  - 하이픈, 특수문자, 따옴표는 사용하지 않음\n",
    "4. 최종 선택: 상위 3~5개만 남김\n",
    "\n",
    "[금지]\n",
    "- 1단어 쿼리 금지\n",
    "- 4단어 이상 금지\n",
    "- 불용어만 남는 조합 금지(ex: “for research”)\n",
    "- JSON 스키마 위반 금지\n",
    "\n",
    "[Input]\n",
    "- 연구 주제: {title}\n",
    "- 연구 설명: {description}\n",
    "- 키워드: {keyword}\n",
    "\n",
    "[Output]\n",
    "다음 형식의 JSON을 출력하세요:\n",
    "{{\n",
    "  \"query\": [],\n",
    "}}\n",
    "'''\n",
    "\n",
    "query_prompt = PromptTemplate.from_template(query_template)\n",
    "\n",
    "# Node\n",
    "def generate_query(state: State):\n",
    "\n",
    "    prompt = query_prompt.invoke(\n",
    "        {\n",
    "            'title': state['title'], \n",
    "            'description': state['description'],\n",
    "            'keyword': state['keyword'],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # sLLM\n",
    "    sllm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "    structured_sllm = sllm.with_structured_output(QueryResult)\n",
    "    res = structured_sllm.invoke(prompt)\n",
    "    query = res.query\n",
    "    print('\\n[query]\\n', query)\n",
    "\n",
    "    return {'query': query}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69681e0b",
   "metadata": {},
   "source": [
    "## 논문 - 쿼리에 따른 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "4d2d5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "\n",
    "def ARTI_search(state: State):\n",
    "    \n",
    "    CLIENT_ID = os.getenv(\"SCIENCEON_CLIENT_ID\")\n",
    "    ARTI_KEY = os.getenv(\"SCIENCEON_API_KEY\")\n",
    "    MAC_ADDRESS = os.getenv(\"MAC_ADDRESS\")\n",
    "\n",
    "    access_token = call_access_token(MAC_ADDRESS, ARTI_KEY, CLIENT_ID)\n",
    "\n",
    "    url = \"https://apigateway.kisti.re.kr/openapicall.do\"\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for query in state['query']:\n",
    "        params = {\n",
    "            \"client_id\": CLIENT_ID,\n",
    "            \"token\": access_token,\n",
    "            \"version\": 1.0,\n",
    "            \"action\": \"search\",\n",
    "            \"target\": \"ARTI\",\n",
    "            \"searchQuery\": transform_query(query),\n",
    "            'curPage': 1, # 현재페이지 번호\n",
    "            'rowCount': 20, # 디스플레이 건수(기본값 10, 최대값 100)\n",
    "        }\n",
    "\n",
    "        res = requests.get(url, params=params, timeout=20)\n",
    "        xml = res.text\n",
    "        tmp = xml_to_df(xml)\n",
    "        tmp[\"query\"] = query\n",
    "        df = pd.concat([df, tmp], ignore_index=True)\n",
    "        \n",
    "    df = df.drop_duplicates(subset='CN')\n",
    "    df.to_csv('../data/search_results_article.csv', index=False, encoding='utf-8')\n",
    "    print('\\n[total article length]\\n', len(df))\n",
    "\n",
    "    cleaned_df = (\n",
    "        df[\n",
    "            ['CN', 'Title', 'Abstract', 'Pubyear', 'Keyword', 'Author', 'ContentURL', 'query']\n",
    "        ]\n",
    "        .rename(\n",
    "            columns={\n",
    "                'CN': 'ID',\n",
    "                'Title': 'title',\n",
    "                'Abstract': 'description',\n",
    "                'Pubyear': 'pubyear',\n",
    "                'Keyword': 'keyword',\n",
    "                'Author': 'author',\n",
    "                'ContentURL': 'URL'\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cleaned_df['category'] = 'article'\n",
    "\n",
    "    search_df = state.get('search_df')\n",
    "    \n",
    "    if search_df is not None and not search_df.empty:\n",
    "        return {'search_df': pd.concat([search_df, cleaned_df], ignore_index=True)}\n",
    "    else:\n",
    "        return {'search_df': cleaned_df}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e60a4",
   "metadata": {},
   "source": [
    "## 데이터셋 - 쿼리에 따른 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "821cafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "import pandas as pd\n",
    "\n",
    "def DATA_search(state: State):\n",
    "    \n",
    "    API_KEY = os.getenv(\"DATAON_SEARCH_API_KEY\")\n",
    "    assert API_KEY and API_KEY.strip(), \"환경변수(DATAON_API_KEY)가 비어있어요!\"\n",
    "\n",
    "    url = \"https://dataon.kisti.re.kr/rest/api/search/dataset/\"\n",
    "    df = pd.DataFrame()\n",
    "    for query in state['query']:\n",
    "        params = {\"key\": API_KEY, \"query\": query, \"from\": 0, \"size\": 20}\n",
    "        # key / CHAR / 필수 / API_KEY\n",
    "        # query / CHAR / 필수 / 검색키워드\n",
    "        # from / CHAR / 옵션 / 페이지시작위치\n",
    "        # size / CHAR / 옵션 / 페이지사이즈\n",
    "\n",
    "        res = requests.get(url, params=params, timeout=20)\n",
    "        data = res.json()\n",
    "        \n",
    "        if \"records\" in data:\n",
    "            tmp = pd.DataFrame(data[\"records\"])\n",
    "            tmp[\"query\"] = query\n",
    "            df = pd.concat([df, tmp], ignore_index=True)\n",
    "\n",
    "    df = df.drop_duplicates(subset='svc_id')\n",
    "    df.to_csv('../data/search_results_dataset.csv', index=False, encoding='utf-8')\n",
    "    print('\\n[total dataset length]\\n', len(df))\n",
    "    \n",
    "    cleaned_df = (\n",
    "        df[\n",
    "            ['svc_id', 'dataset_title_etc_main', 'dataset_expl_etc_main','dataset_pub_dt_pc', 'dataset_kywd_etc_main', 'dataset_creator_etc_main', 'dataset_lndgpg', 'query']\n",
    "        ]\n",
    "        .rename(\n",
    "            columns={\n",
    "                'svc_id': 'ID',\n",
    "                'dataset_title_etc_main': 'title',\n",
    "                'dataset_expl_etc_main': 'description',\n",
    "                'dataset_pub_dt_pc': 'pubyear',\n",
    "                'dataset_kywd_etc_main': 'keyword',\n",
    "                'dataset_creator_etc_main': 'author',\n",
    "                'dataset_lndgpg': 'URL',\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cleaned_df['category'] = 'dataset'\n",
    "    \n",
    "    search_df = state.get('search_df')\n",
    "    \n",
    "    if search_df is not None and not search_df.empty:\n",
    "        return {'search_df': pd.concat([search_df, cleaned_df], ignore_index=True)}\n",
    "    else:\n",
    "        return {'search_df': cleaned_df}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e3140a",
   "metadata": {},
   "source": [
    "## 연관성 점수 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "2de73930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_relevance(state: State):\n",
    "    \n",
    "    df = state['search_df']\n",
    "\n",
    "    targets = ['title', 'description', 'keyword']\n",
    "\n",
    "    MAX_LENGTH = 2000\n",
    "\n",
    "    dfs = {}\n",
    "\n",
    "    for target in targets:\n",
    "        print(f'\\n[embedding_{target}]')\n",
    "        \n",
    "        df[target] = df[target].fillna('')\n",
    "        df[target] = df[target].str.slice(0, MAX_LENGTH)\n",
    "\n",
    "        texts = df[target].tolist()\n",
    "\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "        docs = [Document(page_content=text, metadata={\"ID\": row.ID}) \n",
    "                for text, row in zip(texts, df.itertuples())]\n",
    "\n",
    "        batch_size = 500\n",
    "        stores = []\n",
    "        for i in tqdm(range(0, len(docs), batch_size)):\n",
    "            batch = docs[i:i+batch_size]\n",
    "            store = FAISS.from_documents(batch, embeddings)\n",
    "            stores.append(store)\n",
    "\n",
    "        vectorstore = stores[0]\n",
    "        for s in stores[1:]:\n",
    "            vectorstore.merge_from(s)\n",
    "\n",
    "        query = state[target]\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "        results_with_score = vectorstore.similarity_search_with_score_by_vector(query_embedding, k=20)\n",
    "        \n",
    "        dfs[f'df_{target}'] = pd.DataFrame([\n",
    "            {\n",
    "                \"ID\": r.metadata.get(\"ID\"),\n",
    "                \"relevance\": score,\n",
    "                \"target\": target\n",
    "            }\n",
    "            for r, score in results_with_score\n",
    "        ])\n",
    "        \n",
    "    merged_df = dfs['df_title'].merge(\n",
    "        dfs['df_description'][[\"ID\", \"relevance\"]], \n",
    "        on=\"ID\",\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_title\", \"_desc\")\n",
    "    ).merge(\n",
    "        dfs['df_keyword'][[\"ID\", \"relevance\"]].rename(columns={\"relevance\": \"relevance_key\"}), \n",
    "        on=\"ID\",\n",
    "        how=\"outer\"\n",
    "    )\n",
    "\n",
    "    merged_df = merged_df.fillna(2.0)\n",
    "\n",
    "    merged_df = merged_df.drop_duplicates(subset='ID')\n",
    "\n",
    "    merged_df = merged_df[merged_df['ID'] != state['input_id']]\n",
    "\n",
    "    # 2. 가중치 합산\n",
    "    a, b, c = 10, 3, 1\n",
    "    merged_df[\"relevance_raw\"] = (\n",
    "        merged_df[\"relevance_title\"] * a + \n",
    "        merged_df[\"relevance_desc\"] * b + \n",
    "        merged_df[\"relevance_key\"] * c\n",
    "    ) / (a + b + c)\n",
    "\n",
    "    merged_df[\"relevance\"] = 100 * (1 - merged_df[\"relevance_raw\"] / 2)\n",
    "\n",
    "    result_df = (merged_df[[\"ID\", \"relevance\"]]\n",
    "                .sort_values(\"relevance\", ascending=False)\n",
    "                .reset_index(drop=True)).head(5)\n",
    "\n",
    "    return {'relevance_df': result_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e4b3c",
   "metadata": {},
   "source": [
    "## 추천사유 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "380e754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated\n",
    "from pydantic import Field, BaseModel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Schema\n",
    "class IDRelevance(BaseModel):\n",
    "    relevant_id: Annotated[\n",
    "        list[str],\n",
    "        Field(\n",
    "            ..., \n",
    "            description=(\n",
    "                \"데이터의 ID 목록\"\n",
    "            ), \n",
    "        )\n",
    "    ]\n",
    "    reason: Annotated[\n",
    "        list[str],\n",
    "        Field(\n",
    "            ..., \n",
    "            description=\"각 ID가 선정된 이유를 설명하는 문자열 목록. relevant_id와 인덱스가 일치해야 합니다.\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Prompt\n",
    "reason_template = '''\n",
    "당신은 데이터 과학자입니다. 아래는 연구 데이터 목록입니다.\n",
    "\n",
    "각 데이터 혹은 논문 항목은 다음 컬럼을 가지고 있습니다:\n",
    "- ID: 각 데이터의 고유키\n",
    "- 제목\n",
    "- 설명\n",
    "- 키워드\n",
    "\n",
    "[목표]\n",
    "모든 항목에 대해 해당 항목의 선정 사유를 작성해 주세요.\n",
    "\n",
    "**중요: relevant_id와 reason의 개수는 반드시 동일해야 합니다.**\n",
    "\n",
    "[Input]\n",
    "연구 주제: {title}\n",
    "연구 설명: {description}\n",
    "키워드: {keyword}\n",
    "\n",
    "[Data]\n",
    "데이터 목록:\n",
    "{data}\n",
    "\n",
    "[Output]\n",
    "다음 형식의 JSON을 출력하세요. relevant_id와 reason의 길이가 정확히 일치해야 합니다:\n",
    "\n",
    "{{\n",
    "  \"relevant_id\": [\"ID1\", \"ID2\", \"ID3\"],\n",
    "  \"reason\": [\"이유1\", \"이유2\", \"이유3\"]\n",
    "}}\n",
    "'''\n",
    "\n",
    "reason_prompt = PromptTemplate.from_template(reason_template)\n",
    "\n",
    "# Node\n",
    "def generate_reason(state: State):\n",
    "    print('\\ngenerating reasons...\\n')\n",
    "    df, title, description, keyword = state['search_df'], state['title'], state['description'], state['keyword']\n",
    "    \n",
    "    relevant_ids = state['relevance_df']['ID'].tolist()\n",
    "    filtered_df = df[df['ID'].isin(relevant_ids)]\n",
    "\n",
    "    prompt = reason_prompt.invoke(\n",
    "        {\n",
    "            'title': title, \n",
    "            'description': description,\n",
    "            'keyword': keyword,\n",
    "            'data': filtered_df[['ID', 'title', 'description', 'keyword']].to_dict(orient=\"records\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    sllm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "    structured_sllm = sllm.with_structured_output(IDRelevance)\n",
    "    res = structured_sllm.invoke(prompt)\n",
    "        \n",
    "    tmp = pd.DataFrame({\n",
    "        'ID': res.relevant_id,\n",
    "        'reason': res.reason\n",
    "    })\n",
    "    \n",
    "    relevance_df = pd.merge(\n",
    "        state['relevance_df'],\n",
    "        tmp,\n",
    "        on='ID',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    return {'relevance_df': relevance_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0161095",
   "metadata": {},
   "source": [
    "## 결과 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "7da84af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(state: State):\n",
    "    \n",
    "    merged_df = pd.merge(\n",
    "        state['relevance_df'],\n",
    "        state['search_df'][['ID','category', 'title', 'description', 'URL']],\n",
    "        on='ID',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    result_df = merged_df[['category', 'title', 'description', 'relevance', 'reason', 'URL']]\n",
    "    result_df = result_df.rename(columns={\n",
    "        'category': '구분',\n",
    "        'title': '제목',\n",
    "        'description': '설명',\n",
    "        'relevance': '점수',\n",
    "        'reason': '추천 사유',\n",
    "    })\n",
    "\n",
    "    return {'result_df': result_df}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b6939",
   "metadata": {},
   "source": [
    "# 그래프 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "03cf2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_router(state):\n",
    "\n",
    "    if state['input_category'] == 'article':\n",
    "        return 'article'\n",
    "    \n",
    "    if state['input_category'] == 'dataset':\n",
    "        return 'dataset'\n",
    "\n",
    "    return 'END'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "0d8926b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "def build_graph():\n",
    "    builder = StateGraph(State)\n",
    "    \n",
    "    builder.add_sequence([generate_query, ARTI_search, DATA_search, evaluate_relevance, generate_reason, summarize_results]) \n",
    "    builder.add_node('DATA_browse', DATA_browse)\n",
    "    builder.add_node('ARTI_browse', ARTI_browse)\n",
    "    \n",
    "    builder.add_conditional_edges(\n",
    "        START,\n",
    "        input_router,\n",
    "        {\n",
    "            'article': 'ARTI_browse',\n",
    "            'dataset': 'DATA_browse'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    builder.add_edge('ARTI_browse', 'generate_query')\n",
    "    builder.add_edge('DATA_browse', 'generate_query')\n",
    "    \n",
    "    builder.add_edge('summarize_results', END)\n",
    "    \n",
    "    return builder.compile()\n",
    "\n",
    "graph = build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89ecf795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input_id]\n",
      " 37c0f3d51a130211fe55fe6019cc7914\n",
      "\n",
      "[title]\n",
      " Gravity Core from Antarctic ROSS Sea (RS15-GC76)\n",
      "\n",
      "[description]\n",
      " 2014/2015 Gravity core, Ross Sea (O.Granite Harbor), Antarctic Climate change observation. Location : 76°54.9315'S, 163°21.7774'E Water Depth : 808m Core Length : 3.17m\n",
      "\n",
      "[keyword]\n",
      " EARTH SCIENCE;OCEANS;MARINE SEDIMENTS;SEDIMENTATION\n",
      "\n",
      "[query]\n",
      " ['Antarctic Gravity Core', 'Ross Sea Sediments', 'Marine Sedimentation', 'Climate Change Antarctica', 'Earth Science Core']\n",
      "\n",
      "[total article length]\n",
      " 100\n",
      "\n",
      "[total dataset length]\n",
      " 46\n",
      "\n",
      "[embedding_title]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[embedding_description]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[embedding_keyword]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generating reasons...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "구분",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "제목",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "설명",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "점수",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "추천 사유",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "URL",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "146dc294-5f5a-43d2-a4fa-acce32bc3c72",
       "rows": [
        [
         "0",
         "dataset",
         "Gravity Core from Antarctic ROSS Sea (RS15-GC77)",
         "2014/2015 Gravity core, Ross Sea (O.Granite Harbor), Antarctic Climate change observation. Location : 76°49.2974'S, 163°42.7306'E Water Depth : 754m Core Length : 2.63m",
         "99.77595",
         "이 데이터는 O.Granite Harbor 지역에서 수집된 중력 코어로, 연구 주제와 동일한 지역에서의 기후 변화 관찰을 포함하고 있습니다.",
         "https://dx.doi.org/doi:10.22663/KOPRI-KPDC-00002770.1"
        ],
        [
         "1",
         "dataset",
         "Gravity Core from Antarctic ROSS Sea (RS15-GC80)",
         "2014/2015 Gravity core, Ross Sea (O.Granite Harbor), Antarctic Climate change observation. Location : 76°48.2001'S, 163°55.6482'E Water Depth : 721m Core Length : 4.44m",
         "99.45404",
         "이 데이터는 Ross Sea에서의 중력 코어 샘플로, 연구 주제와 동일한 지역에서 수집되었으며, 기후 변화 관찰과 관련이 있습니다.",
         "https://dx.doi.org/doi:10.22663/KOPRI-KPDC-00002833.1"
        ],
        [
         "2",
         "dataset",
         "Gravity Core from Antarctic ROSS Sea (RS15-GC79)",
         "2014/2015 Gravity core, Ross Sea, Antarctic Climate change observation. Location : 76°00.3801'S, 163°27.3877'E Water Depth : 799m Core Length : 4.30m",
         "99.090996",
         "이 데이터는 Ross Sea에서의 중력 코어 샘플로, 연구 주제와 유사한 기후 변화 관찰을 위한 데이터입니다.",
         "https://dx.doi.org/doi:10.22663/KOPRI-KPDC-00002832.1"
        ],
        [
         "3",
         "dataset",
         "Gravity Core from Antarctic ROSS Sea (RS15-GC72)",
         "2014/2015 Gravity core, Ross Sea (NE.Lewis Bay), Antarctic Climate change observation. Location : 77°14.7225'S, 167°33.7824'E Water Depth : 944m Core Length : 3.63m",
         "98.83951",
         "이 데이터는 Ross Sea의 NE.Lewis Bay 지역에서 수집된 중력 코어로, 기후 변화 관찰과 관련된 중요한 정보를 제공합니다.",
         "https://dx.doi.org/doi:10.22663/KOPRI-KPDC-00002740.1"
        ],
        [
         "4",
         "dataset",
         "Gravity Core from Antarctic ROSS Sea (RS15-GC78)",
         "2014/2015 Gravity core, Ross Sea (Mawson Glacier), Antarctic Climate change observation. Location : 76°15.3160'S, 163°28.5318'E Water Depth : 821m Core Length : 3.93m",
         "98.63711",
         "이 데이터는 Ross Sea의 Mawson Glacier 지역에서 수집된 중력 코어로, 연구 주제와 관련된 기후 변화 연구에 기여할 수 있습니다.",
         "https://dx.doi.org/doi:10.22663/KOPRI-KPDC-00002773.1"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>구분</th>\n",
       "      <th>제목</th>\n",
       "      <th>설명</th>\n",
       "      <th>점수</th>\n",
       "      <th>추천 사유</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset</td>\n",
       "      <td>Gravity Core from Antarctic ROSS Sea (RS15-GC77)</td>\n",
       "      <td>2014/2015 Gravity core, Ross Sea (O.Granite Ha...</td>\n",
       "      <td>99.775948</td>\n",
       "      <td>이 데이터는 O.Granite Harbor 지역에서 수집된 중력 코어로, 연구 주제...</td>\n",
       "      <td>https://dx.doi.org/doi:10.22663/KOPRI-KPDC-000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset</td>\n",
       "      <td>Gravity Core from Antarctic ROSS Sea (RS15-GC80)</td>\n",
       "      <td>2014/2015 Gravity core, Ross Sea (O.Granite Ha...</td>\n",
       "      <td>99.454041</td>\n",
       "      <td>이 데이터는 Ross Sea에서의 중력 코어 샘플로, 연구 주제와 동일한 지역에서 ...</td>\n",
       "      <td>https://dx.doi.org/doi:10.22663/KOPRI-KPDC-000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset</td>\n",
       "      <td>Gravity Core from Antarctic ROSS Sea (RS15-GC79)</td>\n",
       "      <td>2014/2015 Gravity core, Ross Sea, Antarctic Cl...</td>\n",
       "      <td>99.090996</td>\n",
       "      <td>이 데이터는 Ross Sea에서의 중력 코어 샘플로, 연구 주제와 유사한 기후 변화...</td>\n",
       "      <td>https://dx.doi.org/doi:10.22663/KOPRI-KPDC-000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset</td>\n",
       "      <td>Gravity Core from Antarctic ROSS Sea (RS15-GC72)</td>\n",
       "      <td>2014/2015 Gravity core, Ross Sea (NE.Lewis Bay...</td>\n",
       "      <td>98.839508</td>\n",
       "      <td>이 데이터는 Ross Sea의 NE.Lewis Bay 지역에서 수집된 중력 코어로,...</td>\n",
       "      <td>https://dx.doi.org/doi:10.22663/KOPRI-KPDC-000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset</td>\n",
       "      <td>Gravity Core from Antarctic ROSS Sea (RS15-GC78)</td>\n",
       "      <td>2014/2015 Gravity core, Ross Sea (Mawson Glaci...</td>\n",
       "      <td>98.637108</td>\n",
       "      <td>이 데이터는 Ross Sea의 Mawson Glacier 지역에서 수집된 중력 코어...</td>\n",
       "      <td>https://dx.doi.org/doi:10.22663/KOPRI-KPDC-000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        구분                                                제목  \\\n",
       "0  dataset  Gravity Core from Antarctic ROSS Sea (RS15-GC77)   \n",
       "1  dataset  Gravity Core from Antarctic ROSS Sea (RS15-GC80)   \n",
       "2  dataset  Gravity Core from Antarctic ROSS Sea (RS15-GC79)   \n",
       "3  dataset  Gravity Core from Antarctic ROSS Sea (RS15-GC72)   \n",
       "4  dataset  Gravity Core from Antarctic ROSS Sea (RS15-GC78)   \n",
       "\n",
       "                                                  설명         점수  \\\n",
       "0  2014/2015 Gravity core, Ross Sea (O.Granite Ha...  99.775948   \n",
       "1  2014/2015 Gravity core, Ross Sea (O.Granite Ha...  99.454041   \n",
       "2  2014/2015 Gravity core, Ross Sea, Antarctic Cl...  99.090996   \n",
       "3  2014/2015 Gravity core, Ross Sea (NE.Lewis Bay...  98.839508   \n",
       "4  2014/2015 Gravity core, Ross Sea (Mawson Glaci...  98.637108   \n",
       "\n",
       "                                               추천 사유  \\\n",
       "0  이 데이터는 O.Granite Harbor 지역에서 수집된 중력 코어로, 연구 주제...   \n",
       "1  이 데이터는 Ross Sea에서의 중력 코어 샘플로, 연구 주제와 동일한 지역에서 ...   \n",
       "2  이 데이터는 Ross Sea에서의 중력 코어 샘플로, 연구 주제와 유사한 기후 변화...   \n",
       "3  이 데이터는 Ross Sea의 NE.Lewis Bay 지역에서 수집된 중력 코어로,...   \n",
       "4  이 데이터는 Ross Sea의 Mawson Glacier 지역에서 수집된 중력 코어...   \n",
       "\n",
       "                                                 URL  \n",
       "0  https://dx.doi.org/doi:10.22663/KOPRI-KPDC-000...  \n",
       "1  https://dx.doi.org/doi:10.22663/KOPRI-KPDC-000...  \n",
       "2  https://dx.doi.org/doi:10.22663/KOPRI-KPDC-000...  \n",
       "3  https://dx.doi.org/doi:10.22663/KOPRI-KPDC-000...  \n",
       "4  https://dx.doi.org/doi:10.22663/KOPRI-KPDC-000...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input_id = 'JAKO200411922932805'\n",
    "# input_category = 'article'\n",
    "\n",
    "input_id = '37c0f3d51a130211fe55fe6019cc7914'\n",
    "input_category = 'dataset'\n",
    "\n",
    "print('[input_id]\\n', input_id)\n",
    "\n",
    "res = graph.invoke({\n",
    "    'input_id': input_id,\n",
    "    'input_category': input_category,\n",
    "})\n",
    "\n",
    "display(res['result_df'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
