{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39bb3e23",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ì…ë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8834241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ì„ í˜¸ì¶œí•©ë‹ˆë‹¤...with ID: b37f0c9413eeb7c45f6fe31cbe3a41ef\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Meta_API_call' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m svc_id = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33më°ì´í„°ì…‹ id :\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# b37f0c9413eeb7c45f6fe31cbe3a41ef\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33më°ì´í„°ì…‹ì„ í˜¸ì¶œí•©ë‹ˆë‹¤...with ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msvc_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m data = \u001b[43mMeta_API_call\u001b[49m(svc_id)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data)) \u001b[38;5;66;03m# type: dict\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'Meta_API_call' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv # ì…ë ¥ íŒŒíŠ¸ ì¢…í•©ë³¸ì— ë¹¼ë‘˜ ê²ƒ.\n",
    "load_dotenv(override=True)\n",
    "\n",
    "max_invalid = 4\n",
    "invalid_count = 0\n",
    "\n",
    "while True:\n",
    "    choice = input(\"ì…ë ¥ë°ì´í„° ì„ íƒ\\n 1. ë°ì´í„°ì…‹  2. ë…¼ë¬¸\\nìˆ«ì(1 ë˜ëŠ” 2)ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "\n",
    "    if choice == \"1\":\n",
    "        # ë°ì´í„°ì…‹ í˜¸ì¶œ API\n",
    "        svc_id = input(\"ë°ì´í„°ì…‹ id :\") # b37f0c9413eeb7c45f6fe31cbe3a41ef\n",
    "        print(f\"ë°ì´í„°ì…‹ì„ í˜¸ì¶œí•©ë‹ˆë‹¤...with ID: {svc_id}\")\n",
    "        data = Meta_API_call(svc_id)\n",
    "        print(type(data)) # type: dict\n",
    "        break\n",
    "    elif choice == \"2\":\n",
    "        # ë…¼ë¬¸ í˜¸ì¶œ API\n",
    "        print(\"ë…¼ë¬¸ì„ í˜¸ì¶œí•©ë‹ˆë‹¤...\")\n",
    "        pass # ë…¼ë¬¸  í˜¸ì¶œ API\n",
    "        break\n",
    "    else:\n",
    "        invalid_count += 1\n",
    "        remaining = max_invalid - invalid_count\n",
    "        if remaining <= 0:\n",
    "            print(\"ì…ë ¥ ì˜¤ë¥˜ê°€ ë„ˆë¬´ ë§ì•„ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            sys.exit(1)\n",
    "        else:\n",
    "            print(f\"ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. {remaining}ë²ˆ ë‚¨ì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857ef74",
   "metadata": {},
   "source": [
    "## ì—°êµ¬ë°ì´í„° ë©”íƒ€ì •ë³´ ì¡°íšŒ API(í•¨ìˆ˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a80aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "def DATA_browse(API_KEY, svc_id):\n",
    "    API_KEY = os.getenv(\"DATAON_META_API_KEY\")\n",
    "    assert API_KEY and API_KEY.strip(), \"í™˜ê²½ë³€ìˆ˜(DATAON_META_API_KEY)ê°€ ë¹„ì–´ìˆì–´ìš”!\"\n",
    "\n",
    "    url = \"https://dataon.kisti.re.kr/rest/api/search/dataset/\"+svc_id\n",
    "    params = {\"key\": API_KEY}\n",
    "\n",
    "    # print(\"ğŸ” í˜¸ì¶œ URL ë¯¸ë¦¬ë³´ê¸°:\", requests.Request('GET', url, params=params).prepare().url)\n",
    "    res = requests.get(url, params=params, timeout=20)\n",
    "    print(\"HTTP\", res.status_code)\n",
    "    data = res.json()\n",
    "    print(data)\n",
    "    print(len(data['records']))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aae44a5",
   "metadata": {},
   "source": [
    "## ê²€ìƒ‰ ë…¸ë“œ(í•¨ìˆ˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be869b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” í˜¸ì¶œ URL ë¯¸ë¦¬ë³´ê¸°: https://dataon.kisti.re.kr/rest/api/search/dataset/?key=D62EA22E8D9BBAFEFA15D7A0F39FD79F&query=korean+text&from=0&size=5\n",
      "HTTP 200\n",
      "{'response': {'elapsed time': '79 ms', 'status': '200', 'message': 'OK', 'total count': 28, 'type': 'json', 'key': 'D62EA22E8D9BBAFEFA15D7A0F39FD79F', 'query': 'korean text', 'from': 0, 'size': 5}, 'records': [{'svc_id': '2a8dadd5a1fb22affc1db8a345d66de4', 'ctlg_type': '02', 'dataset_type': '02', 'ctlg_type_pc': 'dataset', 'dataset_type_pc': 'í•´ì™¸', 'dataset_pub_dt_pc': '2024', 'dataset_access_type_pc': 'ê³µê°œ', 'file_yn_pc': 'ëœë”©í˜ì´ì§€ì´ë™', 'dataset_cc_license_pc': 'none', 'dataset_main_lang_pc': 'English', 'dataset_sub_lang_pc': 'none', 'cltfm_pc': 'ARDC', 'dataset_title_kor': '', 'dataset_title_etc_main': 'Weak Force 3', 'dataset_title_etc_sub': '', 'dataset_expl_kor': '', 'dataset_expl_etc_main': 'Research background: There has been a significant rise in group-based and collective art practice that challenges conventional methodologies and ideas of individual authorship in art. These enterprises approach art as an expanded form of intellectual activity by capitalizing on group dynamics and collective research. Research contribution: The Weak Force 3 builds upon previous iterations of Weak Force projects by the UFT collaborative group. The artists produced a sound and text installation in response to highly disruptive building works that were occurring next door to Zero Gallery at the time. A high-fidelity sound recording of drilling, pile driving and small-scale explosions from the construction site were resituated back into the gallery space through large-scale speakers. Gallery visitors were given the opportunity to switch the sound off or on by pushing an interactive foot pedal. This sound component was accompanied by contemporary Korean and Mandarin vinyl text, which was adhered to the walls and floor of the gallery. All texts referred to sound either metaphorically, metonymically or literally. Professor Suh Yongsun executed a painted text work on the gallery floor during the opening event, which discussed sound in the context of Confucian philosophy. Research significance: The significance of this research is that group-based practice was used to extend and combine the practices of eight artists working across five different continents. The artistÂ¿s created a site-responsive installation that directly addressed the situation at hand, turning a situation of apparent adversity into a creative discourse.', 'dataset_expl_etc_sub': '', 'dataset_kywd_kor': '', 'dataset_kywd_etc_main': '', 'dataset_kywd_etc_sub': '', 'cltfm_kor': '', 'cltfm_etc': 'ARDC', 'dataset_data_loc': '{coordinates:[0.0,0.0],type:point}', 'dataset_lndgpg': 'https://doi.org/10.25439/rmt.27346197.v1', 'dataset_lndgpg_img': '', 'dataset_lndgpg_thum': '', 'dataset_doi': 'https://doi.org/10.25439/rmt.27346197.v1', 'dataset_etc_attr': '', 'dataset_regist_dt': '2025-04-21 17:39:55', 'dataset_creat_dt': '2024-10-30', 'dataset_mod_dt': '0001-01-01 00:00:00', 'dataset_mnsb_pc': ['Not Assigned'], 'file_frmt_pc': ['none'], 'pjt_prfrm_org_pc': ['none'], 'ministry_pc': ['none'], 'dataset_creator_kor': [], 'dataset_creator_etc_main': [], 'dataset_creator_etc_sub': [], 'pjt_prfrm_org_kor': [], 'pjt_prfrm_org_etc': [], 'ministry_kor': [], 'ministry_etc': [], 'pjt_nm_kor': [], 'pjt_nm_etc': [], 'pjt_mngr_kor': [], 'pjt_mngr_etc': [], 'dataset_cntrbtr_kor': [], 'dataset_cntrbtr_etc': [], 'dataset_pblshr': [], 'dataset_pric': []}, {'svc_id': 'b37f0c9413eeb7c45f6fe31cbe3a41ef', 'ctlg_type': '02', 'dataset_type': '02', 'ctlg_type_pc': 'dataset', 'dataset_type_pc': 'í•´ì™¸', 'dataset_pub_dt_pc': '2024', 'dataset_access_type_pc': 'ê³µê°œ', 'file_yn_pc': 'ëœë”©í˜ì´ì§€ì´ë™', 'dataset_cc_license_pc': 'none', 'dataset_main_lang_pc': 'English', 'dataset_sub_lang_pc': 'none', 'cltfm_pc': 'ARDC', 'dataset_title_kor': '', 'dataset_title_etc_main': 'Architectural Urbanism: Melbourne/Seoul - KTA projects', 'dataset_title_etc_sub': '', 'dataset_expl_kor': '', 'dataset_expl_etc_main': \"BACKGROUND: 'Architectural Urbanism: Melbourne/Seoul' was a two-city exhibition funded by RMIT University and the Korean National University, supported by the Australian Government through the Australian International Cultural Council. Kerstin Thompson Architecture (KTA) showed five works - Carrum Downs Police Station, MUMA Gallery, Lake Conneware House, Napier Street Housing and Royal Botanic Gardens Visitor Centre - as one of ten architectural firms (5 from Melbourne, 5 from Seoul) selected to exhibit. The exhibited work included large scale photographs, working drawings and exegetical text. CONTRIBUTION: The exhibition explored architectural approaches that worked 'within the city rather than upon it', architecture that 'intervenes and inserts, rather than overlays or eradicates'. Within this context, the public projects exhibited by KTA explore relationships between interior and exterior, building and street, and the existing and the new. This draws on Thompson's ongoing practice and design research focussing on architecture as a civic endeavour, forging connections between buildings, their surroundings and the people who inhabit them. SIGNIFICANCE: 'Architectural Urbanism' was an international, cross institutional exhibition that received external funding, evidencing significance for the selected works by the 10 participating practices. In Seoul, the exhibition was opened by the deputy Australian Ambassador. The Melbourne exhibition was reviewed in Architecture Australia (AA). These KTA projects have also received multiple awards and media attention including four AIA (Australian Institute of Architects) Victorian Architecture Awards, an IDEA (Interior Design Excellence Awards) award, three book features, critical reviews in AA, and articles in SMH, The Australian, Broadsheet and Archdaily.\", 'dataset_expl_etc_sub': '', 'dataset_kywd_kor': '', 'dataset_kywd_etc_main': '', 'dataset_kywd_etc_sub': '', 'cltfm_kor': '', 'cltfm_etc': 'ARDC', 'dataset_data_loc': '{coordinates:[0.0,0.0],type:point}', 'dataset_lndgpg': 'https://doi.org/10.25439/rmt.27353883.v1', 'dataset_lndgpg_img': '', 'dataset_lndgpg_thum': '', 'dataset_doi': 'https://doi.org/10.25439/rmt.27353883.v1', 'dataset_etc_attr': '', 'dataset_regist_dt': '2025-04-21 12:02:59', 'dataset_creat_dt': '2024-10-30', 'dataset_mod_dt': '0001-01-01 00:00:00', 'dataset_mnsb_pc': ['Not Assigned'], 'file_frmt_pc': ['none'], 'pjt_prfrm_org_pc': ['none'], 'ministry_pc': ['none'], 'dataset_creator_kor': [], 'dataset_creator_etc_main': [], 'dataset_creator_etc_sub': [], 'pjt_prfrm_org_kor': [], 'pjt_prfrm_org_etc': [], 'ministry_kor': [], 'ministry_etc': [], 'pjt_nm_kor': [], 'pjt_nm_etc': [], 'pjt_mngr_kor': [], 'pjt_mngr_etc': [], 'dataset_cntrbtr_kor': [], 'dataset_cntrbtr_etc': [], 'dataset_pblshr': [], 'dataset_pric': []}, {'svc_id': '6e9bbf90560c0236aa9c1ffdabc51b4f', 'ctlg_type': '02', 'dataset_type': '02', 'ctlg_type_pc': 'dataset', 'dataset_type_pc': 'í•´ì™¸', 'dataset_pub_dt_pc': '2024', 'dataset_access_type_pc': 'ê³µê°œ', 'file_yn_pc': 'ëœë”©í˜ì´ì§€ì´ë™', 'dataset_cc_license_pc': 'CC-BY-4.0', 'dataset_main_lang_pc': 'English', 'dataset_sub_lang_pc': 'none', 'cltfm_pc': 'OpenAIRE', 'dataset_title_kor': '', 'dataset_title_etc_main': 'Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis', 'dataset_title_etc_sub': '', 'dataset_expl_kor': '', 'dataset_expl_etc_main': '&lt;b&gt;Please cite the following paper when using this dataset:&lt;/b&gt; &lt;br&gt; N. Thakur, â€œMpox narrative on Instagram: A labeled multilingual dataset of Instagram posts on mpox for sentiment, hate speech, and anxiety analysis,â€ arXiv [cs.LG], 2024, URL: https://arxiv.org/abs/2409.05292 &lt;br&gt; &lt;br&gt; &lt;b&gt;Abstract&lt;/b&gt; &lt;br&gt; The world is currently experiencing an outbreak of mpox, which has been declared a Public Health Emergency of International Concern by WHO. During recent virus outbreaks, social media platforms have played a crucial role in keeping the global population informed and updated regarding various aspects of the outbreaks. As a result, in the last few years, researchers from different disciplines have focused on the development of social media datasets focusing on different virus outbreaks. No prior work in this field has focused on the development of a dataset of Instagram posts about the mpox outbreak. The work presented in this paper (stated above) aims to address this research gap. It presents this &lt;b&gt;multilingual dataset of 60,127 Instagram posts&lt;/b&gt; about mpox, published between &lt;b&gt;July 23, 2022, and September 5, 2024.&lt;/b&gt; This dataset contains Instagram posts about mpox in &lt;b&gt;52 languages&lt;/b&gt;.   For each of these posts, the Post ID, Post Description, Date of publication, language, and translated version of the post (translation to English was performed using the Google Translate API) are presented as separate attributes in the dataset.  &lt;br&gt;&lt;br&gt; After developing this dataset, sentiment analysis, hate speech detection, and anxiety or stress detection were also performed. This process included classifying each post into  &lt;ul&gt;  &lt;li&gt;one of the fine-grain sentiment classes, i.e., &lt;b&gt;fear, surprise, joy, sadness, anger, disgust, or neutral&lt;/b&gt;&lt;/li&gt;  &lt;li&gt;&lt;b&gt;hate or not hate&lt;/b&gt;&lt;/li&gt;  &lt;li&gt;&lt;b&gt;anxiety/stress detected or no anxiety/stress detected&lt;/b&gt;&lt;/li&gt; &lt;/ul&gt; &lt;br&gt; These results are presented as &lt;b&gt;separate attributes&lt;/b&gt; in the dataset for the training and testing of machine learning algorithms for sentiment, hate speech, and anxiety or stress detection, as well as for other applications.  &lt;br&gt; &lt;br&gt; &lt;b&gt;The distinct languages in which Instagram posts are present in this dataset are &lt;/b&gt; English, Portuguese, Indonesian, Spanish, Korean, French, Hindi, Finnish, Turkish, Italian, German, Tamil, Urdu, Thai, Arabic, Persian, Tagalog, Dutch, Catalan, Bengali, Marathi, Malayalam, Swahili, Afrikaans, Panjabi, Gujarati, Somali, Lithuanian, Norwegian, Estonian, Swedish, Telugu, Russian, Danish, Slovak, Japanese, Kannada, Polish, Vietnamese, Hebrew, Romanian, Nepali, Czech, Modern Greek, Albanian, Croatian, Slovenian, Bulgarian, Ukrainian, Welsh, Hungarian, and Latvian &lt;br&gt; &lt;br&gt; &lt;b&gt;The following is a description of the attributes present in this dataset:&lt;/b&gt; &lt;ul&gt;  &lt;li&gt;&lt;b&gt;Post ID&lt;/b&gt;: Unique ID of each Instagram post&lt;/li&gt;  &lt;li&gt;&lt;b&gt;Post Description&lt;/b&gt;: Complete description of each post in the language in which it was originally published&lt;/li&gt;  &lt;li&gt;&lt;b&gt;Date&lt;/b&gt;: Date of publication in MM/DD/YYYY format&lt;/li&gt;  &lt;li&gt;&lt;b&gt;Language&lt;/b&gt;: Language of the post as detected using the Google Translate API &lt;/li&gt;  &lt;li&gt;&lt;b&gt;Translated Post Description&lt;/b&gt;: Translated version of the post description. All posts which were not in English were translated into English using the Google Translate API. No language translation was performed for English posts.&lt;/li&gt;  &lt;li&gt;&lt;b&gt;Sentiment&lt;/b&gt;: Results of sentiment analysis (using the preprocessed version of the translated Post Description) where each post was classified into one of the sentiment classes: fear, surprise, joy, sadness, anger, disgust, and neutral&lt;/li&gt;   &lt;li&gt;&lt;b&gt;Hate&lt;/b&gt;: Results of hate speech detection (using the preprocessed version of the translated Post Description) where each post was classified as hate or not hate&lt;/li&gt;  &lt;li&gt;&lt;b&gt;Anxiety or Stress&lt;/b&gt;: Results of anxiety or stress detection (using the preprocessed version of the translated Post Description) where each post was classified as stress/anxiety detected or no stress/anxiety detected. &lt;/li&gt; &lt;/ul&gt; &lt;br&gt; &lt;br&gt; All the Instagram posts that were collected during this data mining process to develop this dataset were publicly available on Instagram and did not require a user to log in to Instagram to view the same (at the time of writing this paper).;The dataset can be directly used for training and testing of machine learning algorithms for sentiment, hate speech, and anxiety or stress detection, as well as for other applications.', 'dataset_expl_etc_sub': 'The dataset can be directly used for training and testing of machine learning algorithms for sentiment, hate speech, and anxiety or stress detection, as well as for other applications.', 'dataset_kywd_kor': '', 'dataset_kywd_etc_main': 'text classification;data analysis;Social Sciences;Mathematical Sciences;epidemic;mpox;WHO;Engineering;dataset;syndromic surveillance;emotion analysis;virus outbreak;Computer and Information Science;public discourse;public health;artificial intelligence;neural networks;Google Translate;machine learning;classification;Medicine, Health and Life Sciences;sentiment analysis;Instagram;monkeypox;stress analysis;public perception;language translation;social networks;hate speech;social media;online behavior;text mining;unsupervised learning;supervised learning;NLP;online hate;social media mining;toxic content detection;pandemic studies;toxic language;health communication;information retrieval;natural language processing;multilingual dataset;anxiety detection;language detection;pandemic;health misinformation;pattern recognition;social media platforms;LGBTQ+ stigma;data mining;misinformation analysis;web mining;public attitudes;AI;mpox stigma;online misinformation;Other;data science;social contagion;user-generated content', 'dataset_kywd_etc_sub': 'data analysis', 'cltfm_kor': '', 'cltfm_etc': 'OpenAIRE', 'dataset_data_loc': '{coordinates:[0.0,0.0],type:point}', 'dataset_lndgpg': 'https://dx.doi.org/10.21227/7fvc-y093;https://dx.doi.org/10.5281/zenodo.13738598;https://dx.doi.org/10.6084/m9.figshare.27072247.v1;https://dx.doi.org/10.6084/m9.figshare.27072247;https://dx.doi.org/10.5281/zenodo.13738597;https://dx.doi.org/10.7910/dvn/tjvsy0', 'dataset_lndgpg_img': '', 'dataset_lndgpg_thum': '', 'dataset_doi': '', 'dataset_etc_attr': '', 'dataset_regist_dt': '2025-02-18 12:25:47', 'dataset_creat_dt': '2024-01-01', 'dataset_mod_dt': '0001-01-01 00:00:00', 'dataset_mnsb_pc': ['none'], 'file_frmt_pc': ['none'], 'pjt_prfrm_org_pc': ['none'], 'ministry_pc': ['none'], 'dataset_creator_kor': [], 'dataset_creator_etc_main': ['Thakur, Nirmalya'], 'dataset_creator_etc_sub': [], 'pjt_prfrm_org_kor': [], 'pjt_prfrm_org_etc': [], 'ministry_kor': [], 'ministry_etc': [], 'pjt_nm_kor': [], 'pjt_nm_etc': [], 'pjt_mngr_kor': [], 'pjt_mngr_etc': [], 'dataset_cntrbtr_kor': [], 'dataset_cntrbtr_etc': [], 'dataset_pblshr': [], 'dataset_pric': []}, {'svc_id': '9c723508590d42b85b0f9f773e794ba0', 'ctlg_type': '02', 'dataset_type': '02', 'ctlg_type_pc': 'dataset', 'dataset_type_pc': 'í•´ì™¸', 'dataset_pub_dt_pc': '2024', 'dataset_access_type_pc': 'ê³µê°œ', 'file_yn_pc': 'ëœë”©í˜ì´ì§€ì´ë™', 'dataset_cc_license_pc': 'CC-BY-4.0', 'dataset_main_lang_pc': 'English', 'dataset_sub_lang_pc': 'none', 'cltfm_pc': 'OpenAIRE', 'dataset_title_kor': '', 'dataset_title_etc_main': 'Sapsaree.fasta', 'dataset_title_etc_sub': '', 'dataset_expl_kor': '', 'dataset_expl_etc_main': '<b>Korean Sapsaree </b><b>Illumina 170K CanineHD BeadChip</b><b> Data</b><b><i>Dataset Overview:</i></b> This dataset contains genomic data from a group of Korean Sapsaree dogs. It specifically includes genetic information from 234 individual dogs, each genotyped using the Illumina 170K CanineHD BeadChip.<br><b><i>Files Included:</i></b><b>Sapsaree.fasta:</b> This file contains the genotypic data of the Korean Sapsaree dogs in FASTA format, a widely used text-based format for representing nucleotide sequences. Each entry corresponds to a single dog, with headers that provide identifying information followed by the sequence data.<br><b><i>Purpose of the Dataset:</i></b>The main purpose of releasing this dataset is to enhance the scientific understanding of Sapsaree dogs. By making these data available, we aim to support further research into canine genetics and breeding strategies that can lead to improvements in canine health and traits. Access to the data for reproducing results requires an application to, and permission from, the authors.', 'dataset_expl_etc_sub': '', 'dataset_kywd_kor': '', 'dataset_kywd_etc_main': '', 'dataset_kywd_etc_sub': '', 'cltfm_kor': '', 'cltfm_etc': 'OpenAIRE', 'dataset_data_loc': '{coordinates:[0.0,0.0],type:point}', 'dataset_lndgpg': 'https://dx.doi.org/10.6084/m9.figshare.25769994.v1', 'dataset_lndgpg_img': '', 'dataset_lndgpg_thum': '', 'dataset_doi': '', 'dataset_etc_attr': '', 'dataset_regist_dt': '2025-03-15 05:20:01', 'dataset_creat_dt': '2024-01-01', 'dataset_mod_dt': '0001-01-01 00:00:00', 'dataset_mnsb_pc': ['none'], 'file_frmt_pc': ['none'], 'pjt_prfrm_org_pc': ['none'], 'ministry_pc': ['none'], 'dataset_creator_kor': [], 'dataset_creator_etc_main': ['Haque, Md Azizul', 'Kim, Jong-Joo'], 'dataset_creator_etc_sub': [], 'pjt_prfrm_org_kor': [], 'pjt_prfrm_org_etc': [], 'ministry_kor': [], 'ministry_etc': [], 'pjt_nm_kor': [], 'pjt_nm_etc': [], 'pjt_mngr_kor': [], 'pjt_mngr_etc': [], 'dataset_cntrbtr_kor': [], 'dataset_cntrbtr_etc': [], 'dataset_pblshr': [], 'dataset_pric': []}, {'svc_id': '47362dcdad1390d223faaa7325e37ef9', 'ctlg_type': '02', 'dataset_type': '02', 'ctlg_type_pc': 'dataset', 'dataset_type_pc': 'í•´ì™¸', 'dataset_pub_dt_pc': '2024', 'dataset_access_type_pc': 'ê³µê°œ', 'file_yn_pc': 'ëœë”©í˜ì´ì§€ì´ë™', 'dataset_cc_license_pc': 'CC-BY-4.0', 'dataset_main_lang_pc': 'English', 'dataset_sub_lang_pc': 'none', 'cltfm_pc': 'OpenAIRE', 'dataset_title_kor': '', 'dataset_title_etc_main': '<b>Genomic Prediction and Genome-wide Association Studies in Korean Sapsaree Dogs</b>', 'dataset_title_etc_sub': '', 'dataset_expl_kor': '', 'dataset_expl_etc_main': '<b>Korean Sapsaree </b><b>Illumina 170K CanineHD BeadChip</b><b> Data</b><b><i>Dataset Overview:</i></b> This dataset contains genomic data from a group of Korean Sapsaree dogs. It specifically includes genetic information from 234 individual dogs, each genotyped using the Illumina 170K CanineHD BeadChip.<br><b><i>Files Included:</i></b><b>Sapsaree.fasta:</b> This file contains the genotypic data of the Korean Sapsaree dogs in FASTA format, a widely used text-based format for representing nucleotide sequences. Each entry corresponds to a single dog, with headers that provide identifying information followed by the sequence data.<br><b><i>Purpose of the Dataset:</i></b>The main purpose of releasing this dataset is to enhance the scientific understanding of Sapsaree dogs. By making these data available, we aim to support further research into canine genetics and breeding strategies that can lead to improvements in canine health and traits. Access to the data for reproducing results requires an application to, and permission from, the authors.', 'dataset_expl_etc_sub': '', 'dataset_kywd_kor': '', 'dataset_kywd_etc_main': '', 'dataset_kywd_etc_sub': '', 'cltfm_kor': '', 'cltfm_etc': 'OpenAIRE', 'dataset_data_loc': '{coordinates:[0.0,0.0],type:point}', 'dataset_lndgpg': 'https://dx.doi.org/10.6084/m9.figshare.25769994.v2;https://dx.doi.org/10.6084/m9.figshare.25769994', 'dataset_lndgpg_img': '', 'dataset_lndgpg_thum': '', 'dataset_doi': '', 'dataset_etc_attr': '', 'dataset_regist_dt': '2025-02-18 14:48:12', 'dataset_creat_dt': '2024-01-01', 'dataset_mod_dt': '0001-01-01 00:00:00', 'dataset_mnsb_pc': ['none'], 'file_frmt_pc': ['none'], 'pjt_prfrm_org_pc': ['none'], 'ministry_pc': ['none'], 'dataset_creator_kor': [], 'dataset_creator_etc_main': ['Haque, Md Azizul', 'Kim, Na-Kuang', 'Yeji, Ryu', 'Lee, Bugeun', 'Ha, Ji-Hong', 'Lee, Yun Mi', 'Il, Han-Kook', 'Joo, Kim Jong'], 'dataset_creator_etc_sub': [], 'pjt_prfrm_org_kor': [], 'pjt_prfrm_org_etc': [], 'ministry_kor': [], 'ministry_etc': [], 'pjt_nm_kor': [], 'pjt_nm_etc': [], 'pjt_mngr_kor': [], 'pjt_mngr_etc': [], 'dataset_cntrbtr_kor': [], 'dataset_cntrbtr_etc': [], 'dataset_pblshr': [], 'dataset_pric': []}]}\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# API_KEY = os.getenv(\"DATAON_SEARCH_API_KEY\")\n",
    "# assert API_KEY and API_KEY.strip(), \"í™˜ê²½ë³€ìˆ˜(DATAON_API_KEY)ê°€ ë¹„ì–´ìˆì–´ìš”!\"\n",
    "\n",
    "# url = \"https://dataon.kisti.re.kr/rest/api/search/dataset/\"\n",
    "# params = {\"key\": API_KEY, \"query\": \"korean text\", \"from\": 0, \"size\": 5}\n",
    "# # key / CHAR / í•„ìˆ˜ / API_KEY\n",
    "# # query / CHAR / í•„ìˆ˜ / ê²€ìƒ‰í‚¤ì›Œë“œ\n",
    "# # from / CHAR / ì˜µì…˜ / í˜ì´ì§€ì‹œì‘ìœ„ì¹˜\n",
    "# # size / CHAR / ì˜µì…˜ / í˜ì´ì§€ì‚¬ì´ì¦ˆ\n",
    "\n",
    "# print(\"ğŸ” í˜¸ì¶œ URL ë¯¸ë¦¬ë³´ê¸°:\", requests.Request('GET', url, params=params).prepare().url)\n",
    "# res = requests.get(url, params=params, timeout=20)\n",
    "# print(\"HTTP\", res.status_code)\n",
    "# data = res.json()\n",
    "# print(data)\n",
    "\n",
    "\n",
    "def Search_API_Call(query: str):\n",
    "    API_KEY = os.getenv(\"DATAON_SEARCH_API_KEY\")\n",
    "    assert API_KEY and API_KEY.strip(), \"í™˜ê²½ë³€ìˆ˜(DATAON_API_KEY)ê°€ ë¹„ì–´ìˆì–´ìš”!\"\n",
    "\n",
    "    url = \"https://dataon.kisti.re.kr/rest/api/search/dataset/\"\n",
    "    params = {\"key\": API_KEY, \"query\": \"korean text\", \"from\": 0, \"size\": 5}\n",
    "    # key / CHAR / í•„ìˆ˜ / API_KEY\n",
    "    # query / CHAR / í•„ìˆ˜ / ê²€ìƒ‰í‚¤ì›Œë“œ\n",
    "    # from / CHAR / ì˜µì…˜ / í˜ì´ì§€ì‹œì‘ìœ„ì¹˜\n",
    "    # size / CHAR / ì˜µì…˜ / í˜ì´ì§€ì‚¬ì´ì¦ˆ\n",
    "\n",
    "    # print(\"ğŸ” í˜¸ì¶œ URL ë¯¸ë¦¬ë³´ê¸°:\", requests.Request('GET', url, params=params).prepare().url)\n",
    "    res = requests.get(url, params=params, timeout=20)\n",
    "    print(\"HTTP\", res.status_code)\n",
    "    data = res.json()\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6efddf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP 200\n",
      "response JSON:\n",
      "{\n",
      "  \"errorMessage\": \"(í† í°ë°œê¸‰ìš©)MAC Address í™•ì¸ ë¶ˆê°€\",\n",
      "  \"errorCode\": \"E4006\",\n",
      "  \"statusMessage\": \"Bad Request\",\n",
      "  \"statusCode\": \"400\"\n",
      "}\n",
      "access token í‚¤ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì§€ ëª»í•¨. ìœ„ JSONì„ í™•ì¸í•´ì¤˜.\n"
     ]
    }
   ],
   "source": [
    "# resp ëŠ” ì´ë¯¸ requests.get(...) ê²°ê³¼ë¼ê³  ê°€ì •\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(\"HTTP\", resp.status_code)\n",
    "\n",
    "# 1) JSON íŒŒì‹± ì‹œë„ â€” pretty print\n",
    "try:\n",
    "    data = resp.json()\n",
    "    print(\"response JSON:\")\n",
    "    print(json.dumps(data, ensure_ascii=False, indent=2))\n",
    "\n",
    "    # 2) í† í° ì¶”ì¶œ ì‹œë„(ìì£¼ ì“°ì´ëŠ” í‚¤ ëª©ë¡ ê²€ì‚¬)\n",
    "    candidate_keys = [\"access_token\", \"accessToken\", \"token\", \"accessTokenInfo\", \"result\"]\n",
    "    token = None\n",
    "    for k in candidate_keys:\n",
    "        # ì§ì ‘ í‚¤ê°€ ìˆëŠ” ê²½ìš°\n",
    "        if isinstance(data, dict) and k in data:\n",
    "            # ì§ì ‘ ë¬¸ìì—´ì´ë©´ ì‚¬ìš©, dictì´ë©´ ë” ì°¾ì•„ë´„\n",
    "            v = data[k]\n",
    "            if isinstance(v, str):\n",
    "                token = v; break\n",
    "            if isinstance(v, dict):\n",
    "                # ë‚´ë¶€ì— access_token ìˆì„ ìˆ˜ë„ ìˆìŒ\n",
    "                for subk in (\"access_token\", \"accessToken\", \"token\"):\n",
    "                    if subk in v and isinstance(v[subk], str):\n",
    "                        token = v[subk]; break\n",
    "                if token: break\n",
    "\n",
    "    if token:\n",
    "        print(\"access token (found):\", token)\n",
    "    else:\n",
    "        print(\"access token í‚¤ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì§€ ëª»í•¨. ìœ„ JSONì„ í™•ì¸í•´ì¤˜.\")\n",
    "except ValueError:\n",
    "    # JSON ì•„ë‹˜ -> í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œë„\n",
    "    text = resp.text\n",
    "    print(\"response text (non-json):\")\n",
    "    print(text[:1000])  # ë„ˆë¬´ ê¸¸ë©´ ì¼ë¶€ë§Œ ì¶œë ¥\n",
    "\n",
    "    # ê°„ë‹¨í•œ í† í° íŒ¨í„´(ì˜ˆ: ê¸¸ê³  ëœë¤í•œ ë¬¸ìì—´) íƒìƒ‰(ì„ íƒì )\n",
    "    m = re.search(r'([A-Za-z0-9\\-\\._]{20,200})', text)\n",
    "    if m:\n",
    "        print(\"heuristic token-like match:\", m.group(1))\n",
    "    else:\n",
    "        print(\"í† í° í˜•íƒœì˜ ë¬¸ìì—´ì„ ì°¾ì§€ ëª»í•¨. ì „ì²´ ì‘ë‹µì„ í™•ì¸í•´ë´.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f2197d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "print(type(now))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd260149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6E-BD-E0-6B-BE-DE 20251007230040\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv, dotenv_values\n",
    "\n",
    "load_dotenv(override=True)\n",
    "MAC_ADDRESS = os.getenv('MAC_ADDRESS')\n",
    "print(MAC_ADDRESS, dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766af4d3",
   "metadata": {},
   "source": [
    "ë””ë²„ê¹… ì¤‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65be49af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iv-param test -> 200 https://apigateway.kisti.re.kr/tokenrequest.do?client_id=e37c8354a854dad4e74fe1238b2921f8e47e34ddcaacbd3243d01caa93e874c8&accounts=4o7DiRqSx7BVacjy9RbhdrIV3%2FME%2F19wakmZOLfgBpWMYeu3HpiK%2BzoPKJCeJIDeqiq%2B2npi5pAVIQ0ZZnnUpw%3D%3D&iv=xmRmNXYcpODyH71uOwivlA%3D%3D\n",
      "{\"errorMessage\":\"(í† í°ë°œê¸‰ìš©)MAC Address í™•ì¸ ë¶ˆê°€\",\"errorCode\":\"E4006\",\"statusMessage\":\"Bad Request\",\"statusCode\":\"400\"}\n"
     ]
    }
   ],
   "source": [
    "b64 = base64.b64encode(ciphertext).decode('ascii')        # ciphertextë§Œ\n",
    "iv_b64 = base64.b64encode(iv).decode('ascii')\n",
    "params = {\"client_id\": CLIENT_ID, \"accounts\": b64, \"iv\": iv_b64}\n",
    "r = requests.get(\"https://apigateway.kisti.re.kr/tokenrequest.do\", params=params, timeout=15)\n",
    "print(\"iv-param test ->\", r.status_code, r.url)\n",
    "print(r.text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4f473",
   "metadata": {},
   "source": [
    "# ì•„ë˜ê°€ ì§€ê¸ˆ ì“¸ ì˜ˆì •ì¸ ê²ƒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b86a3c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1C-57-DC-50-BB-31\n",
      "20251008202421\n",
      "len(key_bytes)= 32\n",
      "URL sent: https://apigateway.kisti.re.kr/tokenrequest.do?accounts=%255BD%2523%25DC%2524Xv%25DE%25A4%25A8%25B3%25E0%2592%259D%25E0h%259B%2540%2584%253D%255B%25B3pR%25D4%2590%2520A%25FD.%25F3%25FD%2510%25FE%2580%25A2%25B4J%25C6%258F%2591%2505%2512%2595%2540%2512%253A%25E3%25E8%25AB%2500mvy%2521%25AD%2504%25A3%250E%25C1%250A4%25CB3&client_id=e37c8354a854dad4e74fe1238b2921f8e47e34ddcaacbd3243d01caa93e874c8\n",
      "HTTP 200\n",
      "{\"errorMessage\":\"(í† í°ë°œê¸‰ìš©)MAC Address í™•ì¸ ë¶ˆê°€\",\"errorCode\":\"E4006\",\"statusMessage\":\"Bad Request\",\"statusCode\":\"400\"}\n"
     ]
    }
   ],
   "source": [
    "import os, requests, json, base64\n",
    "from datetime import datetime\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Util.Padding import pad, unpad\n",
    "from dotenv import load_dotenv\n",
    "from urllib import parse\n",
    "\n",
    "load_dotenv(override=True)\n",
    "CLIENT_ID = os.getenv(\"SCIENCEON_CLIENT_ID\")\n",
    "API_KEY = os.getenv(\"SCIENCEON_API_KEY\")  # ë„¤ê°€ ë°œê¸‰ë°›ì€ 32ìë¦¬ ì¸ì¦í‚¤(ë¬¸ìì—´)\n",
    "MAC_RAW = os.getenv(\"MAC_ADDRESS\")\n",
    "\n",
    "# 1) ì •ê·œí™” (í•„ìˆ˜)\n",
    "mac = (MAC_RAW or \"\").strip().upper().replace(\":\", \"-\")\n",
    "if not mac:\n",
    "    raise SystemExit(\"MAC_ADDRESSê°€ ë¹„ì–´ìˆìŒ\")\n",
    "print(mac)\n",
    "\n",
    "# 2) datetime\n",
    "dt = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "print(dt)\n",
    "\n",
    "# 3) accounts json (compact)\n",
    "accounts_json = json.dumps({\"mac_address\": mac, \"datetime\": dt}, separators=(\",\", \":\")).encode()\n",
    "# print(\"accounts_json:\", accounts_json.decode('utf-8'))\n",
    "# print(mac, dt)\n",
    "\n",
    "# 4) key ì¤€ë¹„ (í™•ì¸: key_bytes ê¸¸ì´ ë°˜ë“œì‹œ 16/24/32)\n",
    "key_bytes = bytes(API_KEY, 'utf-8')\n",
    "print(\"len(key_bytes)=\", len(key_bytes))\n",
    "\n",
    "if len(key_bytes) not in (16,24,32):\n",
    "    raise SystemExit(\"API_KEY ê¸¸ì´ ë¬¸ì œ: AES í‚¤ëŠ” 16/24/32 ë°”ì´íŠ¸ì—¬ì•¼ í•¨\")\n",
    "\n",
    "# 5) ì•”í˜¸í™” (ECB)\n",
    "cipher = AES.new(key_bytes, AES.MODE_ECB)\n",
    "ciphertext = cipher.encrypt(pad(accounts_json, AES.block_size))\n",
    "\n",
    "result = parse.quote(ciphertext)\n",
    "params = {\"accounts\": result, \"client_id\": CLIENT_ID}\n",
    "\n",
    "\n",
    "# 8) ìš”ì²­ (requestsê°€ ìë™ URL ì¸ì½”ë”© í•¨)\n",
    "url = \"https://apigateway.kisti.re.kr/tokenrequest.do\"\n",
    "r = requests.get(url, params=params, timeout=15)\n",
    "print(\"URL sent:\", r.url)\n",
    "print(\"HTTP\", r.status_code)\n",
    "print(r.text[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9ac02628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'7cd1621a60d946f4997329ecdfce1df3'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcfd3703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b64 preview: W0Qj3CRYdt6kqLPgkp3gaJtAhD1bs3BS1JAgQf0u8/0Q/oCitErGj5EFEpVAEjrjUDxnvlQYt1CXHjnm\n",
      "endswith '=' : True\n"
     ]
    }
   ],
   "source": [
    "import base64, urllib.parse\n",
    "\n",
    "# b64_std ëŠ” ë„¤ê°€ ì „ì†¡í•œ b64 ë¬¸ìì—´ (íŒ¨ë”© ìœ ì§€)\n",
    "print(\"b64 preview:\", b64_std[:80])\n",
    "print(\"endswith '=' :\", b64_std.endswith(\"=\"))\n",
    "\n",
    "# ì„œë²„ê°€ ë°›ì„ ì‹¤ì œ í…ìŠ¤íŠ¸(ì˜ˆ: requestsê°€ ì „ì†¡í•˜ëŠ” URL ì´í›„ì— ì„œë²„ê°€ ë””ì½”ë”©í•  ê°’)ë¥¼\n",
    "# ì‹œë®¬ë ˆì´ì…˜í•˜ë ¤ë©´ URL ì¸ì½”ë”© í›„ ë‹¤ì‹œ ë””ì½”ë”©í•´ë³´ë©´ ëœë‹¤:\n",
    "encoded = urllib.parse.quote_plus(b64_std)   # requestsê°€ URLì— ë„£ëŠ” í˜•íƒœ\n",
    "decoded = urllib.parse.unquote_plus(encoded)\n",
    "assert decoded == b64_std\n",
    "# base64 ë””ì½”ë”©ìœ¼ë¡œ payloadê°€ ëŒì•„ì˜¤ëŠ”ì§€ í™•ì¸\n",
    "payload = base64.b64decode(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'mac_address': '1C-57-DC-50-BB-31', 'datetime': '20251007234828'}\n"
     ]
    }
   ],
   "source": [
    "t = {\"mac_address\": mac, \"datetime\": dt}\n",
    "tj = json.dumps(t)\n",
    "type(tj)\n",
    "tj\n",
    "\n",
    "# 2) íŒŒì¼ë¡œ ì €ì¥ (ì¶”ì²œ: json.dump, ì‚¬ëŒì´ ì½ê¸° ì¢‹ê²Œ indent ì¶”ê°€)\n",
    "with open(\"data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(t, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 3) íŒŒì¼ì—ì„œ ì½ê¸°\n",
    "with open(\"data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print(type(data))  # <class 'dict'>\n",
    "print(data)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a492d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) accounts JSON ìƒì„±\n",
    "accounts_json = make_accounts_json(MAC, DATETIME)\n",
    "print(\"accounts_json:\", accounts_json)\n",
    "\n",
    "# 2) í‚¤/IV ì¤€ë¹„\n",
    "key = derive_key_from_passphrase(PASSPHRASE)       # 32 bytes (AES-256)\n",
    "iv = b\"\\x00\" * 16                                 # ì˜ˆì‹œ IV (ë¬¸ì„œì— IVê°€ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©)\n",
    "\n",
    "# 3) AES-256-CBC ì•”í˜¸í™” (PKCS7 íŒ¨ë”© ì ìš©)\n",
    "ciphertext = aes256_cbc_encrypt(accounts_json.encode(\"utf-8\"), key, iv)\n",
    "cipher = AES.new(key_bytes, AES.MODE_EBC)\n",
    "    return cipher.encrypt(pad(plaintext_bytes, AES.block_size))\n",
    "\n",
    "# 4) ì•”í˜¸ë¬¸ì„ URL-safe base64ë¡œ ì¸ì½”ë”© (padding ì œê±° ì„ íƒ ê°€ëŠ¥)\n",
    "b64 = base64.urlsafe_b64encode(ciphertext).decode(\"ascii\").rstrip(\"=\")\n",
    "\n",
    "# 5) URI ì¸ì½”ë”© (ë¬¸ì„œì´í–‰ì„± ë•Œë¬¸ì— ì¶”ê°€)\n",
    "accounts_param = urllib.parse.quote_plus(b64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c3d32cc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be aligned to block boundary in ECB mode",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mm05G3Thk9yD7+TLsb2VyS5SCg5JLVDB6/pggbVlkROEw1DkX1MdlevFM2LZzmuAeWrMKxzrpTg2Cuc2tpbILc8bBKKdXxUJhK09uM3vHCls=\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m decrypted_text = unpad(\u001b[43mcipher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecrypt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, AES.block_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/kisti_data_ai/kisti_recommender/venv/lib/python3.13/site-packages/Crypto/Cipher/_mode_ecb.py:196\u001b[39m, in \u001b[36mEcbMode.decrypt\u001b[39m\u001b[34m(self, ciphertext, output)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result == \u001b[32m3\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mData must be aligned to block boundary in ECB mode\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mError \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m while decrypting in ECB mode\u001b[39m\u001b[33m\"\u001b[39m % result)\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Data must be aligned to block boundary in ECB mode"
     ]
    }
   ],
   "source": [
    "text = \"m05G3Thk9yD7+TLsb2VyS5SCg5JLVDB6/pggbVlkROEw1DkX1MdlevFM2LZzmuAeWrMKxzrpTg2Cuc2tpbILc8bBKKdXxUJhK09uM3vHCls=\"\n",
    "\n",
    "decrypted_text = unpad(cipher.decrypt(text.encode('utf-8')), AES.block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c026bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Crypto.Cipher import AES\n",
    "from Crypto.Util.Padding import unpad\n",
    "import base64\n",
    "\n",
    "text = \"m05G3Thk9yD7+TLsb2VyS5SCg5JLVDB6/pggbVlkROEw1DkX1MdlevFM2LZzmuAeWrMKxzrpTg2Cuc2tpbILc8bBKKdXxUJhK09uM3vHCls=\"\n",
    "ciphertext = base64.b64decode(text)   # â† ì¤‘ìš”: base64 ë””ì½”ë”©\n",
    "\n",
    "print(\"len(ciphertext) =\", len(ciphertext), \"mod 16 =\", len(ciphertext) % 16)\n",
    "\n",
    "key = b\"your16bytekey!!\"  # 16/24/32 ë°”ì´íŠ¸ì—¬ì•¼ í•¨\n",
    "cipher = AES.new(key, AES.MODE_ECB)\n",
    "\n",
    "plaintext = unpad(cipher.decrypt(ciphertext), AES.block_size)\n",
    "print(plaintext.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mac = (MAC_ADDRESS or \"\").strip().strip('\"').strip(\"'\").upper().replace(\":\", \"-\")\n",
    "if not mac:\n",
    "    raise SystemExit(\"MAC_ADDRESSê°€ ë¹„ì–´ìˆìŒ\")\n",
    "print(mac)\n",
    "\n",
    "# datetime ìƒì„±\n",
    "dt = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "print(dt)\n",
    "\n",
    "# JSON í˜ì´ë¡œë“œ ìƒì„±\n",
    "payload = {\n",
    "    \"mac_address\": mac,\n",
    "    \"datetime\": dt\n",
    "}\n",
    "plain_json = json.dumps(payload, separators=(',', ':'))\n",
    "\n",
    "# AES ì•”í˜¸í™” â†’ Base64\n",
    "aes = AESTestClass(plain_txt=plain_json, key=API_KEY)\n",
    "b64_cipher = aes.encrypt()\n",
    "\n",
    "# ì¸ì½”ë”© + í† í° ìš”ì²­\n",
    "endpoint = \"https://apigateway.kisti.re.kr/tokenrequest.do\"\n",
    "params = {\n",
    "    \"accounts\": b64_cipher,\n",
    "    \"client_id\": CLIENT_ID\n",
    "}\n",
    "\n",
    "print(\"ğŸ” í˜¸ì¶œ URL ë¯¸ë¦¬ë³´ê¸°:\", requests.Request('GET', endpoint, params=params).prepare().url)\n",
    "response = requests.get(endpoint, params=params, timeout=10)\n",
    "response.raise_for_status()\n",
    "print(\"HTTP\", response.status_code)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.text)\n",
    "\n",
    "data = response.json()\n",
    "type(data)\n",
    "token = data['access_token']\n",
    "token\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
